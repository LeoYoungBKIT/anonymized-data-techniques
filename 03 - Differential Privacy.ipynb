{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Differential Privacy\n",
    "\n",
    "[Differential privacy](https://www.microsoft.com/en-us/research/publication/differential-privacy/) is a popular mechanism to quantitatively assess the privacy loss of a given probabilistic query schema or data transformation method. The fundamental equation of differntial privacy is given as\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "    \\mathrm{Pr}[\\mathcal{K}(D_1) \\in S] \\le exp(\\epsilon) \\times \\mathrm{Pr}[\\mathcal{K}(D_2) \\in S] \\label{eq:dp}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $\\mathrm{Pr}[\\mathcal{K}(D_1) \\in S]$ is the probability of a\n",
    "randomized function $\\mathcal{K}$ to yield one of values in the $S$ when evaluating\n",
    "it on a given dataset $D_1$. The right side is identical to the left except\n",
    "that the function is now evaluated on a dataset $D_2$ that differs from $D_1$\n",
    "in at most one element. And finally, $\\epsilon$ is a parameter that describes\n",
    "how much information is leaked (or generated) by the function.\n",
    "\n",
    "Sounds pretty abstract, so let's work out a simple example: Let's assume we want to build a differentially private dataset from the adult data that we've looked at in part 02. The goal here is to protect an adversary from gaining too much information about the sensitive attribute (income > 50k or not) when adding that person's data to the dataset. With differential privacy, we look at the state of the dataset before and after a person was added and quantify the privacy loss as given by equation ($\\ref{eq:dp}$). The scheme we're evaluating here is a so-called **randomized response method**, which works as follows:\n",
    "\n",
    "* With probability $1-p$, we add a person's true attribute value to the database.\n",
    "* With probability $p$ we choose a random boolean (0/1) value from a distribution returning $0$ with probability $k$ and $1$ with probability $1-k$ and add that value to the database instead.\n",
    "\n",
    "Using this scheme, an attacker cannot know with certainty if the real attribute value of the person or a random one was added to the database. This protects the privacy of the person but of course it also adds noise to the database, making it more difficult to use for legitimate users as well.\n",
    "\n",
    "In practice, we therefore always need to weigh privacy against utility when employing differential privacy. In this notebook, we will calculate the $\\epsilon$ and other relevant parameters for our scheme above and see how we can use this differentially private data to make predictions about the income distribution of the people in our dataset.\n",
    "\n",
    "#### Calculating $\\epsilon$\n",
    "\n",
    "In our differentially private scheme, the probability of adding the true attribute value to the database is $1-p$. The probability of adding a random value is therefore $p$ and the probability of that value being $0$ is $k$. So how can we relate this to eq. (\\ref{eq:dp})? Well, we can set $D_1$ and $D_2$ as the versions of our database **before** and **after** adding the person's data to it. Let's say that before adding the person's data there are $n$ $1$'s in the database. We can then use a query $\\mathcal{K}$ that returns the number of 1's in the database and choose our result set as $S = \\{n\\}$. Before adding the person to the database, $\\mathcal{K}(D_1)=n$ with certainty, hence $\\mathrm{Pr}(\\mathcal{K}(D_1))=1$. After adding the person's data, the probability that the query result is still $n$ can be calculated as follows, depending on the person's attribute value:\n",
    "\n",
    "* If a persons's attribute value is $0$, the probability that $\\mathcal{K}$ is unchanged after adding the data to the database is given as $1-p+p\\cdot k$.\n",
    "* If a person's attribute value is $1$, the probability that $\\mathcal{K}$ is unchanged after adding the data to the database is given as $p\\cdot k$.\n",
    "\n",
    "We therefore have the two equations\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{Pr}[\\mathcal{K}(D_1) \\in S | x_i=1] & = & 1 \\le \\exp{\\epsilon}\\cdot \\mathrm{Pr}[\\mathcal{K}(D_2) \\in S | x_i=1] = \\exp{\\epsilon}\\cdot p \\cdot k \\\\\n",
    "\\mathrm{Pr}[\\mathcal{K}(D_1) \\in S | x_i=0] & = & 1 \\le \\exp{\\epsilon}\\cdot \\mathrm{Pr}[\\mathcal{K}(D_2) \\in S | x_i=0] = \\exp{\\epsilon}\\cdot (1-p+p \\cdot k) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "This yields\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " \\epsilon & \\ge & -\\ln{\\left(p \\cdot k\\right)} \\\\\n",
    " \\epsilon & \\ge & -\\ln{\\left(1-p+p\\cdot k\\right)} \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Since we're interested in an upper bound for $\\epsilon$ and since $-\\ln{\\left(1-p+p\\cdot k\\right)} \\le -\\ln{p\\cdot k}$, we obtain\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\epsilon = -\\ln{\\left(p\\cdot k\\right)}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "**Write a function that returns the value of epsilon for a given $p$ and $k$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../solutions/differential-privacy/epsilon.py\"\n",
    "\n",
    "def epsilon(p, k):\n",
    "    \"\"\"\n",
    "    :param p: The probability of returning a random value instead of the true one\n",
    "    :param k: The probability of returning 1 when generating a random value\n",
    "    :returns: The epsilon for the given values of p, k\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "** Plot $\\epsilon$ for various values of $p$ and $k$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Different Scheme\n",
    "\n",
    "Let's assume we propose the following anonymization scheme for our dataset:\n",
    "\n",
    "* With probability $1-p$, we add a person's true attribute value to the database\n",
    "* With probability $p$, we do not add anything to the database\n",
    "\n",
    "Can you calculate the $\\epsilon$ of this scheme? Which scheme do you prefer, and why? Does this scheme always provide \"plausible deniability\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load \"../solutions/differential-privacy/different-scheme.md\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this tell us?\n",
    "\n",
    "Calculating the $\\epsilon$ is great, but what does it actually tell us about the privacy loss or risk for our use case? Let's assume an adversary want to learn about the real value of a person's attribute. If she knows the model used for generating the data, she could then use Bayesian reasoning to calculate the probability of a person's attribute being $1$ given the observed difference in the database, which we call $\\Delta$. Using Bayes theorem we can calculate this as (for $\\Delta = 1$ here)\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    P(x_i=1 | \\Delta = 1) = P(\\Delta = 1| x_i = 1)\\cdot \\frac{P(x_i=1)}{P(\\Delta=0)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For our scheme, we know that \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    P(\\Delta = 1 | x_i = \\mathrm{1}) = (1-p) + p\\cdot(1-k) = 1-pk\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    P(\\Delta = 1) = (1-p)\\cdot P(x_i = \\mathrm{1}) + p\\cdot(1-k)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "so we obtain\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    P(x_i=1 | \\Delta = 1) = \\frac{(1-pk)\\cdot P(x_i = \\mathrm{1})}{(1-p)\\cdot P(x_i = \\mathrm{1})+p\\cdot(1-k)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Let's see how this relates to $\\epsilon$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "**Write a function that calculates thecondition probability as given in eq. (4).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../solutions/differential-privacy/conditional-prob.py\"\n",
    "\n",
    "def p_cond(p, k, p_1):\n",
    "    \"\"\"\n",
    "    :param   p: The probability of returning a random value instead of the true one\n",
    "    :param   k: The probability of returning 1 when generating a random value\n",
    "    :param p_1: The probability of a person to have an attribute value x_i=1\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "** Choose a given k (e.g. 0.5) as well as a value for P(x_i=yes) and plot the conditional probability from eq. (4) as a function of p.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing It\n",
    "\n",
    "Now that we have a feeling for our scheme we can implement it! For that, we load the \"adult census\" data from the k-anonymity case study again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = (\n",
    "    'age',\n",
    "    'workclass', #Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "    'fnlwgt', # \"weight\" of that person in the dataset (i.e. how many people does that person represent) -> https://www.kansascityfed.org/research/datamuseum/cps/coreinfo/keyconcepts/weights\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'income',\n",
    ")\n",
    "categorical = set((\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "    'race',\n",
    "    'income',\n",
    "))\n",
    "df = pd.read_csv(\"../data/k-anonymity/adult.all.txt\", sep=\", \", header=None, names=names, index_col=False, engine='python');# We load the data using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "**Implement a function that processes a new value according to the differentially private scheme discussed above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../solutions/differential-privacy/process-value.py\"\n",
    "\n",
    "import random\n",
    "\n",
    "def process_value(value, p, k):\n",
    "    \"\"\"\n",
    "    :param value: The value to apply the differentially private scheme to.\n",
    "    :param     p: The probability of returning a random value instead of the true one\n",
    "    :param     k: The probability of returning 1 when generating a random value\n",
    "    :    returns: A new, differentially private value\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "**Now apply this method to the \"income\" column of the adult dataset to obtain a differentially private dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../solutions/differential-privacy/apply.py\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "p = 0.8\n",
    "k = 0.4\n",
    "\n",
    "df['income_binary'] = np.where(df['income'] == '<=50k', 0, 1)\n",
    "df['income_dp'] = 0\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Differentially Private Data\n",
    "\n",
    "After collecting the differentially private data, we want of course to make use of it! For example, we might want to estimate the probability of a person having an income > 50T\\$ based on the data we've collected, which we assume is [Bernoulli distributed](https://en.wikipedia.org/wiki/Bernoulli_distribution) with a probability $p_{1}$. Now, when adding up the data from $n$ persons, the resulting value is [binomially distributed](https://en.wikipedia.org/wiki/Bernoulli_distribution). The mean of this distribution is given as $\\mathrm{E}_{1} = n\\cdot p_{1}$ and the variance as $\\mathrm{Var}_1 = n\\cdot p_1 \\cdot (1-p_1)$. A consistent and unbiased estimator of $\\mathrm{E}_1$ is $\\hat{\\mathrm{E}}_{1} = \\sum_i x_{1}^i$, which then gives an estimate for $p_{1}$ of $\\hat{p}_{1} = \\hat{\\mathrm{E}}_1/n$.\n",
    "\n",
    "Now, if we apply the differential privacy mechanism to our dataset, the probability of obtaining a $1$ will change to $p_{1,dp} = (1-p)\\cdot p_{1}+p\\cdot(1-k)$. Therefore, an unbiased and consistent estimator of $p_1$ based on $p_{1,dp}$ is given as\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{p}_1 = \\frac{\\hat{p}_{1,dp}-p\\cdot(1-k)}{1-p}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "As before, $\\hat{p}_{1,dp}=\\sum_i x_{1,dp}^i/n$. Note that this naive estimator can return a negative probability, which can be avoided by using a more suitable method like a maximum likelihood estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "**Write an estimator for $\\hat{p}_1$ based on a differentially private dataset with parameters $p$ and $k$..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../solutions/differential-privacy/p-1.py\"\n",
    "\n",
    "def p_1_estimator(p_1dp, p, k):\n",
    "    \"\"\"\n",
    "    :param p_1dp: The empirical probability of x_i=1 of our DP dataset.\n",
    "    :param     p: The p value of our DP scheme.\n",
    "    :param     k: The k value of our DP scheme.\n",
    "    :    returns: An estimate of p_1 of our DP dataset.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "**Apply the estimator to the differentially private dataset created above to generate an estimate of $p_1$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../solutions/differential-privacy/estimate-p1.py\"\n",
    "\n",
    "p_1_hat = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "**Write a function that estimates the variance of $\\hat{p}_{1}$ and calculate its value for the case above.**\n",
    "\n",
    "Hint: The variance of $\\hat{p}_1$ can be estimated as $$\\hat{\\mathrm{Var}}_1 = \\frac{\\hat{\\mathrm{Var}}_{1,dp}}{(1-p)^2} = \\frac{\\hat{p}_{1,dp}\\cdot(1-\\hat{p}_{1,dp})}{(1-p)^2\\cdot n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00012764584739878977"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load \"../solutions/differential-privacy/estimate-var.py\"\n",
    "\n",
    "def var_1_estimator(p_1dp, n, p, k):\n",
    "    \"\"\"\n",
    "    :param p_1dp: The empirical probability of x_i=1 of our DP dataset.\n",
    "    :param     n: The number of samples in our dataset.\n",
    "    :param     p: The p value of our DP scheme.\n",
    "    :param     k: The k value of our DP scheme.\n",
    "    :    returns: An estimate of the variance of our DP dataset.\n",
    "    \"\"\"\n",
    "\n",
    "var_1_hat = var_1_estimator(p_1dp, len(df), p, k)\n",
    "var_1_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "** Repeat the data generation process $N$ (e.g. 500) times. For each resulting dataset, estimate $\\hat{p}_1$ and store the value in a list, so that we can plot it later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../solutions/differential-privacy/repeat-dp.py\"\n",
    "\n",
    "p_1_hats = []\n",
    "\n",
    "for j in range(500):\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    p_1_hats.append(p_1_hat)\n",
    "\n",
    "p_1_hats = np.array(p_1_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then compare these estimates to the expected distribution (via central limit theorem: a normal distribution\n",
    "# with expectation p_1 and variance var_1_hat)\n",
    "\n",
    "import matplotlib.pylab as pl\n",
    "\n",
    "pl.hist(p_1_hats, density=True)\n",
    "\n",
    "gauss = lambda x, mu, var: 1/np.sqrt(2*np.pi*var)*np.exp(-(x-mu)**2/(2*var))\n",
    "\n",
    "p_1_hat = p_1_hats.mean()\n",
    "\n",
    "x = np.linspace(0.1, 0.3, 1000)\n",
    "\n",
    "pl.plot(x, gauss(x, p_1_hat, var_1_hat));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "That's it! As you can see, applying a differential privacy mechanism to your data is not so difficult, you have to take into account the added noise though, which will make your estimates less precise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
